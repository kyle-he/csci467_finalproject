{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of the dataset:\n",
      "   Unnamed: 0           created_at  default_profile  default_profile_image  \\\n",
      "0           0  2016-10-15 21:32:11            False                  False   \n",
      "1           1  2016-11-09 05:01:30            False                  False   \n",
      "2           2  2017-06-17 05:34:27            False                  False   \n",
      "3           3  2016-07-21 13:32:25             True                  False   \n",
      "4           4  2012-01-15 16:32:35            False                  False   \n",
      "\n",
      "                                         description  favourites_count  \\\n",
      "0  Blame @xaiax, Inspired by @MakingInvisible, us...                 4   \n",
      "1  Photographing the American West since 1980. I ...               536   \n",
      "2  Scruffy looking nerf herder and @twitch broadc...              3307   \n",
      "3  Wife.Godmother.Friend.Feline Fanatic! Assistan...              8433   \n",
      "4               Loan coach at @mancity & Aspiring DJ                88   \n",
      "\n",
      "   followers_count  friends_count  geo_enabled                  id lang  \\\n",
      "0             1589              4        False  787405734442958848   en   \n",
      "1              860            880        False  796216118331310080   en   \n",
      "2              172            594         True  875949740503859204   en   \n",
      "3              517            633         True  756119643622735875   en   \n",
      "4           753678            116         True           464781334   en   \n",
      "\n",
      "                  location                      profile_background_image_url  \\\n",
      "0                  unknown  http://abs.twimg.com/images/themes/theme1/bg.png   \n",
      "1           Estados Unidos  http://abs.twimg.com/images/themes/theme1/bg.png   \n",
      "2          Los Angeles, CA  http://abs.twimg.com/images/themes/theme1/bg.png   \n",
      "3           Birmingham, AL                                               NaN   \n",
      "4  England, United Kingdom  http://abs.twimg.com/images/themes/theme1/bg.png   \n",
      "\n",
      "                                   profile_image_url      screen_name  \\\n",
      "0  http://pbs.twimg.com/profile_images/7874121826...  best_in_dumbest   \n",
      "1  http://pbs.twimg.com/profile_images/8023296328...     CJRubinPhoto   \n",
      "2  http://pbs.twimg.com/profile_images/1278890453...         SVGEGENT   \n",
      "3  http://pbs.twimg.com/profile_images/1284884924...    TinkerVHELPK5   \n",
      "4  http://pbs.twimg.com/profile_images/9952566258...    JoleonLescott   \n",
      "\n",
      "   statuses_count  verified  average_tweets_per_day  account_age_days  \\\n",
      "0           11041     False                   7.870              1403   \n",
      "1             252     False                   0.183              1379   \n",
      "2            1001     False                   0.864              1159   \n",
      "3            1324     False                   0.889              1489   \n",
      "4            4202      True                   1.339              3138   \n",
      "\n",
      "  account_type  \n",
      "0          bot  \n",
      "1        human  \n",
      "2        human  \n",
      "3        human  \n",
      "4        human  \n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.utils import resample\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data acquisition\n",
    "url = \"https://huggingface.co/datasets/airt-ml/twitter-human-bots/resolve/main/twitter_human_bots_dataset.csv\"\n",
    "response = requests.get(url)\n",
    "with open(\"twitter_human_bots_dataset.csv\", \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "# Read the CSV file using pandas\n",
    "df = pd.read_csv(\"twitter_human_bots_dataset.csv\")\n",
    "\n",
    "# Data inspection\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Handle missing values\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# Encode target variable\n",
    "le = LabelEncoder()\n",
    "df['account_type_encoded'] = le.fit_transform(df['account_type'])  # 'bot' -> 0, 'human' -> 1\n",
    "\n",
    "# Convert boolean columns\n",
    "boolean_cols = ['default_profile', 'default_profile_image', 'geo_enabled', 'verified']\n",
    "for col in boolean_cols:\n",
    "    df[col] = df[col].astype(int)\n",
    "\n",
    "# Encode categorical features\n",
    "df['lang_encoded'] = le.fit_transform(df['lang'])\n",
    "\n",
    "# Feature selection\n",
    "df_model = df.drop(columns=[\n",
    "    'Unnamed: 0', 'created_at', 'description', 'id',\n",
    "    'profile_background_image_url', 'profile_image_url',\n",
    "    'screen_name', 'location', 'account_type', 'lang'\n",
    "])\n",
    "\n",
    "# Define features and target\n",
    "X = df_model.drop('account_type_encoded', axis=1)\n",
    "y = df_model['account_type_encoded']\n",
    "\n",
    "# Addressing class imbalance in the training data\n",
    "# Separate majority and minority classes\n",
    "df_majority = df_model[df_model['account_type_encoded'] == 1]  # Human\n",
    "df_minority = df_model[df_model['account_type_encoded'] == 0]  # Bot\n",
    "\n",
    "# Downsample majority class\n",
    "df_majority_downsampled = resample(df_majority, \n",
    "                                   replace=False,            # sample without replacement\n",
    "                                   n_samples=len(df_minority),  # match minority class size\n",
    "                                   random_state=42)\n",
    "\n",
    "# Combine minority class with downsampled majority class\n",
    "df_balanced = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "# Separate features and target for balanced dataset\n",
    "X_balanced = df_balanced.drop('account_type_encoded', axis=1)\n",
    "y_balanced = df_balanced['account_type_encoded']\n",
    "\n",
    "# Data standardization\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_balanced)\n",
    "\n",
    "# Train-test split with balanced data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_balanced, test_size=0.2, stratify=y_balanced, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.5763, Validation Loss: 0.5073\n",
      "Epoch 2, Train Loss: 0.5201, Validation Loss: 0.4855\n",
      "Epoch 3, Train Loss: 0.5069, Validation Loss: 0.4791\n",
      "Epoch 4, Train Loss: 0.5018, Validation Loss: 0.4735\n",
      "Epoch 5, Train Loss: 0.4994, Validation Loss: 0.4684\n",
      "Epoch 6, Train Loss: 0.4917, Validation Loss: 0.4636\n",
      "Epoch 7, Train Loss: 0.4884, Validation Loss: 0.4624\n",
      "Epoch 8, Train Loss: 0.4852, Validation Loss: 0.4617\n",
      "Epoch 9, Train Loss: 0.4803, Validation Loss: 0.4596\n",
      "Epoch 10, Train Loss: 0.4771, Validation Loss: 0.4569\n",
      "Epoch 11, Train Loss: 0.4759, Validation Loss: 0.4550\n",
      "Epoch 12, Train Loss: 0.4770, Validation Loss: 0.4517\n",
      "Epoch 13, Train Loss: 0.4721, Validation Loss: 0.4522\n",
      "Epoch 14, Train Loss: 0.4702, Validation Loss: 0.4547\n",
      "Epoch 15, Train Loss: 0.4673, Validation Loss: 0.4504\n",
      "Epoch 16, Train Loss: 0.4727, Validation Loss: 0.4490\n",
      "Epoch 17, Train Loss: 0.4682, Validation Loss: 0.4482\n",
      "Epoch 18, Train Loss: 0.4721, Validation Loss: 0.4483\n",
      "Epoch 19, Train Loss: 0.4672, Validation Loss: 0.4483\n",
      "Epoch 20, Train Loss: 0.4677, Validation Loss: 0.4489\n",
      "Epoch 21, Train Loss: 0.4689, Validation Loss: 0.4473\n",
      "Epoch 22, Train Loss: 0.4637, Validation Loss: 0.4455\n",
      "Epoch 23, Train Loss: 0.4683, Validation Loss: 0.4460\n",
      "Epoch 24, Train Loss: 0.4644, Validation Loss: 0.4462\n",
      "Epoch 25, Train Loss: 0.4619, Validation Loss: 0.4458\n",
      "Epoch 26, Train Loss: 0.4637, Validation Loss: 0.4446\n",
      "Epoch 27, Train Loss: 0.4613, Validation Loss: 0.4434\n",
      "Epoch 28, Train Loss: 0.4623, Validation Loss: 0.4426\n",
      "Epoch 29, Train Loss: 0.4626, Validation Loss: 0.4423\n",
      "Epoch 30, Train Loss: 0.4616, Validation Loss: 0.4436\n",
      "Epoch 31, Train Loss: 0.4592, Validation Loss: 0.4425\n",
      "Epoch 32, Train Loss: 0.4575, Validation Loss: 0.4411\n",
      "Epoch 33, Train Loss: 0.4612, Validation Loss: 0.4422\n",
      "Epoch 34, Train Loss: 0.4603, Validation Loss: 0.4426\n",
      "Epoch 35, Train Loss: 0.4583, Validation Loss: 0.4396\n",
      "Epoch 36, Train Loss: 0.4592, Validation Loss: 0.4394\n",
      "Epoch 37, Train Loss: 0.4550, Validation Loss: 0.4383\n",
      "Epoch 38, Train Loss: 0.4582, Validation Loss: 0.4391\n",
      "Epoch 39, Train Loss: 0.4558, Validation Loss: 0.4395\n",
      "Epoch 40, Train Loss: 0.4549, Validation Loss: 0.4384\n",
      "Epoch 41, Train Loss: 0.4555, Validation Loss: 0.4370\n",
      "Epoch 42, Train Loss: 0.4532, Validation Loss: 0.4381\n",
      "Epoch 43, Train Loss: 0.4568, Validation Loss: 0.4393\n",
      "Epoch 44, Train Loss: 0.4533, Validation Loss: 0.4394\n",
      "Epoch 45, Train Loss: 0.4530, Validation Loss: 0.4383\n",
      "Early stopping at epoch 46\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.85      0.81      1272\n",
      "           1       0.83      0.74      0.79      1272\n",
      "\n",
      "    accuracy                           0.80      2544\n",
      "   macro avg       0.80      0.80      0.80      2544\n",
      "weighted avg       0.80      0.80      0.80      2544\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1083  189]\n",
      " [ 326  946]]\n"
     ]
    }
   ],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.to_numpy(), dtype=torch.float32).view(-1, 1)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.to_numpy(), dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Create DataLoader objects for batching\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define the model architecture\n",
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(FeedForwardNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.dropout1 = nn.Dropout(0.4)  # Increased dropout\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.dropout2 = nn.Dropout(0.4)  # Increased dropout\n",
    "        self.output = nn.Linear(32, 1)\n",
    "        self.sigmoid = nn.Sigmoid()  # Sigmoid for binary classification\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.sigmoid(self.output(x))\n",
    "        return x\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = FeedForwardNN(input_size=X_train.shape[1])\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop with early stopping\n",
    "num_epochs = 50\n",
    "patience = 5\n",
    "best_loss = float('inf')\n",
    "early_stop_count = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_val, y_val in test_loader:\n",
    "            outputs = model(X_val)\n",
    "            loss = criterion(outputs, y_val)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    # Early stopping check\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_model = model.state_dict()\n",
    "        early_stop_count = 0\n",
    "    else:\n",
    "        early_stop_count += 1\n",
    "        if early_stop_count >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss/len(train_loader):.4f}, Validation Loss: {val_loss/len(test_loader):.4f}\")\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(best_model)\n",
    "\n",
    "# Evaluation on test set\n",
    "model.eval()\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for X_batch, _ in test_loader:\n",
    "        outputs = model(X_batch)\n",
    "        y_pred.extend(outputs.squeeze().numpy())\n",
    "\n",
    "y_pred_binary = (np.array(y_pred) > 0.5).astype(int)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_binary))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_binary))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "467_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
